<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>kula41 Tech Space, share technology post</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>PHP网站301定向方法详解</title>
				<description>&lt;p&gt;PHP网站301定向方法详解&lt;/p&gt;

&lt;p&gt;关于301重定向的好处，和一般301重定向的方法，网上有很多，笔者就不重复了，今天要说的，是PHP网站通过修改PHP网页的形式做好301重定向。
前段时间，笔者网站更换域名，在做301重定向时，搜索了网上很多帖子，发现很多都罗列了各种方法，却都没有具体将怎么操作，比如PHP网站，就是写出下面一段代码了事&lt;/p&gt;

&lt;p&gt;PHP代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;? Header( &quot;HTTP/1.1 301 Moved Permanently&quot; );
Header( &quot;Location: http://www.gcidc.net&quot; );?&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个代码只适应于网站迁移的情况，即旧站和新站域名不同、根目录也不相同的情况。如果网站属于一个网站绑定2个域名，2个域名的根目录为同一个，那么，就会行成死循环。在此，笔者将PHP网站301重定向方法完全放出，希望能帮到新人，如果有不足之处，也请高手指出，不胜感激。&lt;/p&gt;

&lt;p&gt;这个代码只适应于网站首页的301重定向，如果网站已经被收录，或存在大量的非首页外链，要全部定向过来，那就无能为力了。更合适的方法是使用.htaccess来定向，简单快捷，不过笔者不懂这个，在此，笔者将PHP网站301重定向方法完全放出，希望能帮到新人，如果有不足之处，也请高手指出，不胜感激。&lt;/p&gt;

&lt;p&gt;一、建立一个301.inc.php（文件名自取）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$the_host = $_SERVER['HTTP_HOST'];//取得进入所输入的域名
$request_uri = isset($_SERVER['REQUEST_URI']) ? $_SERVER['REQUEST_URI'] : '';//判断地址后面部分
if($the_host !== 'www.gcidc.net')//这是我要以前的域名地址
{
  header('HTTP/1.1 301 Moved Permanently');//发出301头部 
  header('Location: http://www.xxx.net'.$request_uri);//跳转到我的新域名地址
}
?&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中if($the_host !== ‘www.gcidc.net’)中的“!==”是不完全等于的意思，也可以用“!=”不等于，这样，就可以将以前的域名，包括gcxirang.com、www.xxx.com以及新域名中我gcidc.net全部重定向到www.xxx.net。&lt;/p&gt;

&lt;p&gt;二、在网页文件中调用301.inc.php&lt;/p&gt;

&lt;p&gt;将这个代码放到你所有网页的最前面。&lt;/p&gt;

&lt;p&gt;好了，重定向就做好了，将301文件上传到网站目录，就可以去看着你的成果了。&lt;/p&gt;
</description>
				<pubDate>Tue, 26 May 2015 00:00:00 +0800</pubDate>
				<link>/web/2015/05/26/web-301.html</link>
				<guid isPermaLink="true">/web/2015/05/26/web-301.html</guid>
			</item>
		
			<item>
				<title>自动生成26个英文字母的序列</title>
				<description>&lt;p&gt;自动生成26个英文字母的序列&lt;/p&gt;

&lt;p&gt;=SUBSTITUTE(ADDRESS(1,MOD(ROW(),270),4),1,””)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://files.c.excelhome.net/forum/201109/23/1126258xd08uxg9dlc9d00.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://files.c.excelhome.net/forum/201109/23/113652w8dwxne7fgo7xz2o.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 16 May 2015 00:00:00 +0800</pubDate>
				<link>/excel/2015/05/16/excel-char.html</link>
				<guid isPermaLink="true">/excel/2015/05/16/excel-char.html</guid>
			</item>
		
			<item>
				<title>独立主机IIS6.0环境下配置rewrite（discuz为例）</title>
				<description>&lt;p&gt;独立主机IIS6.0环境下配置rewrite（discuz为例）&lt;/p&gt;

&lt;p&gt;discuz论坛，网址静态化对网站收录以及优化都有着重要的意义！那么，怎么实现经验化呢？下面，我们就详细的操作一下让大家看看！
1、进入seo设置页面，将会面的全部打钩钩！然后确认！&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://s4.sinaimg.cn/middle/9cbcfbcctx6Czzn8nV9c3&amp;amp;690&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2、查看当前的 Rewrite 规则，根据自己的实际使用情况选择合适自己的代码！（本人使用的是windows2003,IIS6.0的服务器！）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://s12.sinaimg.cn/middle/9cbcfbcctx6Czzn9Rurbb&amp;amp;690&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3、复制代码，在文本文档中另存为！　
windows 主机 保存为   httpd.ini
linux虚拟主机 保存为    htaccess
然后上传至根目录！&lt;/p&gt;

&lt;p&gt;4、空间需要支持开通rewrite重写！
http://pan.baidu.com/share/link?shareid=279966250&amp;amp;uk=1460636505  点击下载软件！
（以下文章，引自http://www.hexiaojun.com/143.html，感谢无私分享）
　　下面来一步步开始安装：
　　我们先将将下载的 IIS Rewrite 组件解压，放到适当的目录（如C:\ISAPI_Rewrite3）下。
&lt;img src=&quot;http://s5.sinaimg.cn/large/9cbcfbcctx6CzASz1pqb4&amp;amp;690&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;5、解压完毕后，您需要给C:\ISAPI_Rewrite3目录加上Users的读和运行权限，不然可能会造成IIS无法启动。&lt;/p&gt;

&lt;p&gt;6、设置好权限后，在 IIS 管理器里选择网站，右键选择“属性”，如下图所示：
&lt;img src=&quot;http://s15.sinaimg.cn/large/9cbcfbcctx6CzB3Et7o4e&amp;amp;690&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;7、然后选择ISAPI筛选器，然后点击“添加” 选型卡 如下图
&lt;img src=&quot;http://s15.sinaimg.cn/large/9cbcfbcctx6CzB2MlgG1e&amp;amp;690&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;8、　　点击添加选项卡之后，在筛选器名称填写iiswrite，可执行文件选取：C:\ISAPI_Rewrite3\ISAPI_Rewrite.dll ，也就是解压isapi_rewrite 3的文件夹路径。如下图：
&lt;img src=&quot;http://s9.sinaimg.cn/large/9cbcfbcctx6CzB6mxegc8&amp;amp;690&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;9、点击“确定” 按钮。
&lt;img src=&quot;http://s5.sinaimg.cn/large/9cbcfbcctx6CzBd6D7m24&amp;amp;690&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;10、　　重新启动 IIS
&lt;img src=&quot;http://s3.sinaimg.cn/large/9cbcfbcctx6CzBeX1bsf2&amp;amp;690&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;11、　　点击确定。
&lt;img src=&quot;http://s10.sinaimg.cn/large/9cbcfbcctx6CzBhDMoNb9&amp;amp;690&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;12、　　重新选择网站 =&amp;gt; 右键“属性”=&amp;gt; “ISAPI 筛选器”，如果看到状态为向上的绿色箭头，就说明 IISRewrite 模块安装成功了。
&lt;img src=&quot;http://s10.sinaimg.cn/large/9cbcfbcctx6CzBhDMoNb9&amp;amp;690&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;　　注意：我们服务器系统是win2003，安装的都是IIS的web服务器，不是Apache的，按照本教程配置好VPS的伪静态后，您只需要把您网站的静态化规则文件命名为httpd.ini文件放在网站根目录也就是web目录下就可以了，无须通过网站后台设置，那是在Apache服务器下才那样设置的，有的shopex的程序需要先把httpd.ini放web下再在后台启用下伪静态设置。&lt;/p&gt;
</description>
				<pubDate>Thu, 30 Apr 2015 00:00:00 +0800</pubDate>
				<link>/seo/2015/04/30/iis-rewrite.html</link>
				<guid isPermaLink="true">/seo/2015/04/30/iis-rewrite.html</guid>
			</item>
		
			<item>
				<title>织梦（DEDECMS）网站程序及数据库迁移搬家</title>
				<description>&lt;p&gt;织梦（DEDECMS）网站程序及数据库迁移搬家&lt;/p&gt;

&lt;p&gt;首先我们将目的地的数据库账号密码获取到，并且保证持有一个数据库或拥有创建数据库的权限。&lt;/p&gt;

&lt;p&gt;1、首先登录织梦的后台，进入到“系统》数据库备份/还原”栏目。&lt;/p&gt;

&lt;p&gt;2、如果之前有备份过数据，就需要将织梦程序根目录下的data文件夹下的backupdata文件夹改名为其他名称，如”_backupdata”。&lt;/p&gt;

&lt;p&gt;3、全选所有的表，根据情况选择MYSQL版本，按照默认选项提交备份。&lt;/p&gt;

&lt;p&gt;4、备份成功后，将网站所有程序和目录上传到新的服务器。(主要文件有backupdata）&lt;/p&gt;

&lt;p&gt;5、然后将已上传程序根目录中install文件夹下的“index.php.bak”改名为“index.php”，并且将“module-install.php.bak”更改为“module-install.php”，删除“install_lock.txt”文件。&lt;/p&gt;

&lt;p&gt;6、在浏览器地址栏输入“你的域名/install/index.php”开始正常安装程序，其中数据库账号密码需填写目的地服务器的资料，网站程序的账号密码保持默认。&lt;/p&gt;

&lt;p&gt;7、登录网站程序后台“http://域名/后台路径”，进入到“系统》数据库备份/还原》右上角数据还原》左下角开始还原数据”。&lt;/p&gt;

&lt;p&gt;8、当数据完整的还原后，把网站程序中的install文件夹权限设置为不可读或直接删掉。&lt;/p&gt;
</description>
				<pubDate>Tue, 21 Apr 2015 00:00:00 +0800</pubDate>
				<link>/seo/2015/04/21/web-migrate.html</link>
				<guid isPermaLink="true">/seo/2015/04/21/web-migrate.html</guid>
			</item>
		
			<item>
				<title>六大免费网站数据采集器对比</title>
				<description>&lt;p&gt;六大免费网站数据采集器对比(火车头,海纳,云采集,ET,三人行,狂人采集)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. 火车头

火车头应该是国内采集软件成功的典型之一，使用人数包括收费用户数量上应该是最多的。
特色：功能强大，速度快，支持的网站最丰富，支持丰富扩展。
优点：功能齐全，采集速度比较快，主要针对CMS，短时间可以采集很多，过滤，替换都不错，比较详细；很多人写接口、规则和发布模块，接口比较齐全；支持的扩展非常好用，如果你是一位懂技术的站长，可以使用PHP或C#开发任意功能的扩展；附件采集功能完善。
缺点：采集规则编写对很多站长是个不小的门槛。功能增多，软件越来越大，比较占用内存和CPU资源，资源回收控制得不好。另外，授权绑定计算机，有时很不方便。只能在Windows平台下使用，没有Linux版本。
技术：技术主要是论坛支持，帮助文件多。有收费、免费版本。


2. 海纳

特色：关键词抓取，可以预览采集内容，不用写规则。
优点：可以抓取网站很多一个关键词文章，似乎适合做网站的专题，特别是文章类、博客类。
缺点：分类不方便，即采集文章归类不方便，要手动（自动容易混淆），特定接口，采集的内容有限，一次只能采集一条，无法批量采集，需要和网站后台网页对接。安装时，需要海纳的人员上门技术支持，比较麻烦。
技术：无论坛。收费，免费的功能限制太大，形同鸡肋。

3. 云采集

特色：完美无缝的集合了火车头和海纳的优点，功能强大，速度快，关键词抓取，不用写规则。提供基于网络的接口供第三方调用，创新给力。
优点：功能强大，不需要写任何规则，软件使用简单，多线程，速度快，可以多个关键词采集，可以批量采集批量入库，傻瓜式采集，可以定时采集和发布，无人值守，适合做网站专题。能够和任意CMS，如PHP、ASP.NET(C#)、JSP、Ruby等开发的CMS紧密整合。和网站后台频道无缝对接，方便文章发布。安装简单，支持Windows和Linux。
缺点：虽然也较为有名，但与火车头和海纳相比，发展时间相对较短，相对新锐，偶尔采集的内容不太准确，不过很容易校正调整。
技术：QQ技术支持、论坛、微博。有永久免费版本、收费版本。收费版本也可以通过嵌入代码资源交换方式免费使用，很灵活。

4. ET工具

特色：无人值守，稳定，资源占用最低，基本上可以叫安静。
优点：无人值守，自动更新，用户群主要集中在长期做站潜水站长。软件清晰，必备功能也很齐全，软件免费，听说已经增加采集中英文翻译功能。
缺点：对论坛和CMS的支持一般。
技术：论坛支持，软件本身免费，但是也提供收费服务。帮助文件较少，上手不容易。

5. 三人行

主要针对论坛的采集，功能比较完善。先申明，不知道三人行和狂人是什么关系，但界面和功能都是一个模子出来的。
特色：针对各大论坛，搬家，移动，速度快，准确度高。
优点：还是针对论坛，适合开论坛的。
技术：收费技术，免费有广告。
缺点：超级复杂，上手难，对CMS支持比较差。

6. 狂人

特色：可以让你的新论坛一开始就会有大量的会员。
优点：非常适合采集discuz论坛。
缺点：过于专一，兼容性不好。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;总结：追求简单易用，功能较为齐全的，可以选择云采集。追求功能非常齐全的，可以选择火车头。云采集和火车头都能迅速采集很多的资源，充实网站内容。如果做论坛，那选择三人行，可以实现采集论坛，回复，搬家等多项论坛功能。长期做站，可以选择ET或云采集，花点时间，弄懂，是个长期受益的事情，他们都可以像开QQ一样，长期运行，不费内存，自动采集更新。至于海纳，似乎不写规则，上手容易，但是对文章的发布上，比较麻烦。另外，这里只讲了六大主要的采集工具，其实还有网络矿工、网络神采、易采、gooseeker、soukey、小猪采集器、超级采集、千帆采集等等，这些采集器也是各有优缺点，但总体上来说属于采集工具领域的第二梯队，就不一一再讲了。。。。。。&lt;/p&gt;
</description>
				<pubDate>Tue, 21 Apr 2015 00:00:00 +0800</pubDate>
				<link>/seo/2015/04/21/seo-collet.html</link>
				<guid isPermaLink="true">/seo/2015/04/21/seo-collet.html</guid>
			</item>
		
			<item>
				<title>curl 命令行下载工具使用方法小结</title>
				<description>&lt;p&gt;curl 命令行下载工具使用方法小结&lt;/p&gt;

&lt;p&gt;1) 
二话不说，先从这里开始吧！ 
curl http://www.yahoo.com&lt;/p&gt;

&lt;p&gt;回车之后，www.yahoo.com 的html就稀里哗啦地显示在屏幕上了~~~~~&lt;/p&gt;

&lt;p&gt;2) 
嗯，要想把读过来页面存下来，是不是要这样呢？ 
curl http://www.yahoo.com &amp;gt; page.html&lt;/p&gt;

&lt;p&gt;当然可以，但不用这么麻烦的！ 
用curl的内置option就好，存下http的结果，用这个option: -o 
curl -o page.html http://www.yahoo.com&lt;/p&gt;

&lt;p&gt;这样，你就可以看到屏幕上出现一个下载页面进度指示。等进展到100%，自然就OK咯&lt;/p&gt;

&lt;p&gt;3) 
什么什么？！访问不到？肯定是你的proxy没有设定了。 
使用curl的时候，用这个option可以指定http访问所使用的proxy服务器及其端口： -x 
curl -x 123.45.67.89:1080 -o page.html http://www.yahoo.com&lt;/p&gt;

&lt;p&gt;4) 
访问有些网站的时候比较讨厌，他使用cookie来记录session信息。 
像IE/NN这样的浏览器，当然可以轻易处理cookie信息，但我们的curl呢？….. 
我们来学习这个option: -D &amp;lt;– 这个是把http的response里面的cookie信息存到一个特别的文件中去 
curl -x 123.45.67.89:1080 -o page.html -D cookie0001.txt http://www.yahoo.com&lt;/p&gt;

&lt;p&gt;这样，当页面被存到page.html的同时，cookie信息也被存到了cookie0001.txt里面了&lt;/p&gt;

&lt;p&gt;5） 
那么，下一次访问的时候，如何继续使用上次留下的cookie信息呢？要知道，很多网站都是靠监视你的cookie信息， 
来判断你是不是不按规矩访问他们的网站的。 
这次我们使用这个option来把上次的cookie信息追加到http request里面去： -b 
curl -x 123.45.67.89:1080 -o page1.html -D cookie0002.txt -b cookie0001.txt http://www.yahoo.com&lt;/p&gt;

&lt;p&gt;这样，我们就可以几乎模拟所有的IE操作，去访问网页了！&lt;/p&gt;

&lt;p&gt;6） 
稍微等等~~~~~我好像忘记什么了~~~~~ 
对了！是浏览器信息~~~~&lt;/p&gt;

&lt;p&gt;有些讨厌的网站总要我们使用某些特定的浏览器去访问他们，有时候更过分的是，还要使用某些特定的版本~~~~ 
NND，哪里有时间为了它去找这些怪异的浏览器呢！？&lt;/p&gt;

&lt;p&gt;好在curl给我们提供了一个有用的option，可以让我们随意指定自己这次访问所宣称的自己的浏览器信息： -A 
curl -A “Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0)” -x 123.45.67.89:1080 -o page.html -D cookie0001.txt http://www.yahoo.com&lt;/p&gt;

&lt;p&gt;这样，服务器端接到访问的要求，会认为你是一个运行在Windows 2000上的IE6.0，嘿嘿嘿，其实也许你用的是苹果机呢！&lt;/p&gt;

&lt;p&gt;而”Mozilla/4.73 [en] (X11; U; Linux 2.2; 15 i686”则可以告诉对方你是一台PC上跑着的Linux，用的是Netscape 4.73，呵呵呵&lt;/p&gt;

&lt;p&gt;7） 
另外一个服务器端常用的限制方法，就是检查http访问的referer。比如你先访问首页，再访问里面所指定的下载页，这第二次访问的 referer地址就是第一次访问成功后的页面地址。这样，服务器端只要发现对下载页面某次访问的referer地址不是首页的地址，就可以断定那是个盗连了~~~~~&lt;/p&gt;

&lt;p&gt;讨厌讨厌~~~我就是要盗连~~~~~！！ 
幸好curl给我们提供了设定referer的option： -e 
curl -A “Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0)” -x 123.45.67.89:1080 -e “mail.yahoo.com” -o page.html -D cookie0001.txt http://www.yahoo.com&lt;/p&gt;

&lt;p&gt;这样，就可以骗对方的服务器，你是从mail.yahoo.com点击某个链接过来的了，呵呵呵&lt;/p&gt;

&lt;p&gt;8） 
写着写着发现漏掉什么重要的东西了！—– 利用curl 下载文件&lt;/p&gt;

&lt;p&gt;刚才讲过了，下载页面到一个文件里，可以使用 -o ，下载文件也是一样。 
比如， curl -o 1.jpg http://cgi2.tky.3web.ne.jp/~zzh/screen1.JPG 
这里教大家一个新的option： -O 
大写的O，这么用： curl -O http://cgi2.tky.3web.ne.jp/~zzh/screen1.JPG 
这样，就可以按照服务器上的文件名，自动存在本地了！&lt;/p&gt;

&lt;p&gt;再来一个更好用的。 
如果screen1.JPG以外还有screen2.JPG、screen3.JPG、….、screen10.JPG需要下载，难不成还要让我们写一个script来完成这些操作？ 
不干！ 
在curl里面，这么写就可以了： 
curl -O http://cgi2.tky.3web.ne.jp/~zzh/screen[1-10].JPG&lt;/p&gt;

&lt;p&gt;呵呵呵，厉害吧？！~~~&lt;/p&gt;

&lt;p&gt;9） 
再来，我们继续讲解下载！ 
curl -O http://cgi2.tky.3web.ne.jp/~/[001-201].JPG&lt;/p&gt;

&lt;p&gt;这样产生的下载，就是 
~zzh/001.JPG 
~zzh/002.JPG 
… 
~zzh/201.JPG 
~nick/001.JPG 
~nick/002.JPG 
… 
~nick/201.JPG&lt;/p&gt;

&lt;p&gt;够方便的了吧？哈哈哈&lt;/p&gt;

&lt;p&gt;咦？高兴得太早了。 
由于zzh/nick下的文件名都是001，002…，201，下载下来的文件重名，后面的把前面的文件都给覆盖掉了~~~&lt;/p&gt;

&lt;p&gt;没关系，我们还有更狠的！ 
curl -o #2_#1.jpg http://cgi2.tky.3web.ne.jp/~/[001-201].JPG&lt;/p&gt;

&lt;p&gt;–这是…..自定义文件名的下载？ 
–对头，呵呵！&lt;/p&gt;

&lt;h1 id=&quot;zzhnick&quot;&gt;1是变量，指的是这部分，第一次取值zzh，第二次取值nick&lt;/h1&gt;
&lt;p&gt;#2代表的变量，则是第二段可变部分—[001-201]，取值从001逐一加到201 
这样，自定义出来下载下来的文件名，就变成了这样： 
原来： ~zzh/001.JPG —&amp;gt; 下载后： 001-zzh.JPG 
原来： ~nick/001.JPG —&amp;gt; 下载后： 001-nick.JPG&lt;/p&gt;

&lt;p&gt;这样一来，就不怕文件重名啦，呵呵&lt;/p&gt;

&lt;p&gt;9） 
继续讲下载 
我们平时在windows平台上，flashget这样的工具可以帮我们分块并行下载，还可以断线续传。 
curl在这些方面也不输给谁，嘿嘿&lt;/p&gt;

&lt;p&gt;比如我们下载screen1.JPG中，突然掉线了，我们就可以这样开始续传 
curl -c -O http://cgi2.tky.3wb.ne.jp/~zzh/screen1.JPG&lt;/p&gt;

&lt;p&gt;当然，你不要拿个flashget下载了一半的文件来糊弄我~~~~别的下载软件的半截文件可不一定能用哦~~~&lt;/p&gt;

&lt;p&gt;分块下载，我们使用这个option就可以了： -r 
举例说明 
比如我们有一个http://cgi2.tky.3web.ne.jp/~zzh/zhao1.mp3 要下载（赵老师的电话朗诵 :D ） 
我们就可以用这样的命令： 
curl -r 0-10240 -o “zhao.part1” http:/cgi2.tky.3web.ne.jp/~zzh/zhao1.mp3 &amp;amp;\ 
curl -r 10241-20480 -o “zhao.part1” http:/cgi2.tky.3web.ne.jp/~zzh/zhao1.mp3 &amp;amp;\ 
curl -r 20481-40960 -o “zhao.part1” http:/cgi2.tky.3web.ne.jp/~zzh/zhao1.mp3 &amp;amp;\ 
curl -r 40961- -o “zhao.part1” http:/cgi2.tky.3web.ne.jp/~zzh/zhao1.mp3&lt;/p&gt;

&lt;p&gt;这样就可以分块下载啦。 
不过你需要自己把这些破碎的文件合并起来 
如果你用UNIX或苹果，用 cat zhao.part* &amp;gt; zhao.mp3就可以 
如果用的是Windows，用copy /b 来解决吧，呵呵&lt;/p&gt;

&lt;p&gt;上面讲的都是http协议的下载，其实ftp也一样可以用。 
用法嘛， 
curl -u name:passwd ftp://ip:port/path/file 
或者大家熟悉的 
curl ftp://name:passwd@ip:port/path/file&lt;/p&gt;

&lt;p&gt;10) 
说完了下载，接下来自然该讲上传咯 
上传的option是 -T&lt;/p&gt;

&lt;p&gt;比如我们向ftp传一个文件： curl -T localfile -u name:passwd ftp://upload_site:port/path/&lt;/p&gt;

&lt;p&gt;当然，向http服务器上传文件也可以 
比如 curl -T localfile http://cgi2.tky.3web.ne.jp/~zzh/abc.cgi 
注意，这时候，使用的协议是HTTP的PUT method&lt;/p&gt;

&lt;p&gt;刚才说到PUT，嘿嘿，自然让老服想起来了其他几种methos还没讲呢！ 
GET和POST都不能忘哦。&lt;/p&gt;

&lt;p&gt;http提交一个表单，比较常用的是POST模式和GET模式&lt;/p&gt;

&lt;p&gt;GET模式什么option都不用，只需要把变量写在url里面就可以了 
比如： 
curl http://www.yahoo.com/login.cgi?user=nickwolfe&amp;amp;password=12345&lt;/p&gt;

&lt;p&gt;而POST模式的option则是 -d&lt;/p&gt;

&lt;p&gt;比如，curl -d “user=nickwolfe&amp;amp;password=12345” http://www.yahoo.com/login.cgi 
就相当于向这个站点发出一次登陆申请~~~~~&lt;/p&gt;

&lt;p&gt;到底该用GET模式还是POST模式，要看对面服务器的程序设定。&lt;/p&gt;

&lt;p&gt;一点需要注意的是，POST模式下的文件上的文件上传，比如&lt;/p&gt;
&lt;form method=&quot;POST&quot; enctype=&quot;multipar/form-data&quot; action=&quot;http://cgi2.tky.3web.ne.jp/~zzh/up_file.cgi&quot;&gt; 
&amp;lt;input type=file name=upload&amp;gt; 
&amp;lt;input type=submit name=nick value=&quot;go&quot;&amp;gt; 
&lt;/form&gt;
&lt;p&gt;这样一个HTTP表单，我们要用curl进行模拟，就该是这样的语法： 
curl -F upload=@localfile -F nick=go http://cgi2.tky.3web.ne.jp/~zzh/up_file.cgi&lt;/p&gt;

&lt;p&gt;罗罗嗦嗦讲了这么多，其实curl还有很多很多技巧和用法 
比如 https的时候使用本地证书，就可以这样 
curl -E localcert.pem https://remote_server&lt;/p&gt;

&lt;p&gt;再比如，你还可以用curl通过dict协议去查字典~~~~~ 
curl dict://dict.org/d:computer&lt;/p&gt;
</description>
				<pubDate>Mon, 20 Apr 2015 00:00:00 +0800</pubDate>
				<link>/linux/2015/04/20/linux-curl.html</link>
				<guid isPermaLink="true">/linux/2015/04/20/linux-curl.html</guid>
			</item>
		
			<item>
				<title>如何让百度快速收录</title>
				<description>&lt;p&gt;如何让百度快速收录&lt;/p&gt;

&lt;p&gt;我们都知道，一个网站，要先有收录，才有排名，有了排名，才有流量。很多学员问我这样一个问题，为什么一个新站上线很久，百度却迟迟不收，我一个一个回答的都累了，那我们今天就在这里详细讲解一下新站如何实现让百度秒收的几个注意事项。&lt;/p&gt;

&lt;p&gt;网站上线后,千万不要到百度URL的提交入口去提交,因为你一旦提交了，那么你就被百度对于新站的算法判定为你提交的这是一个新站，而百度对于新站的算法相对于百度对蜘蛛爬行到的链接算法是有很大差异的。对于新站的算法，我们这里暂且称它为《百度新站算法》。百度对于新站短时间内不会马上收录的,而是会有一个评估时间,这个时间段内加入百度觉得你的网站内容是有价值的,那么它就会把这个时间段缩短从而提前收录;但是它如果觉得这个站的内容一般情况,也不会完全是抄袭的它就会等评估时间过后再收录;另外一种情况是倘若这个站是在很差，完全原封不动照抄的,那它就会把这个站判断为较差质量的网站，百度的这个评估时间段一般为30天左右。所以新手千万不要心急，新站上线后不管它，直接扔外链，因为百度对于蜘蛛爬行到的外链而进行的直接质量评估,远比你提交获得的算法来的块。百度通过外链质量的算法,如果计算出的得分较高,它就会立即收录。&lt;/p&gt;

&lt;p&gt;那么在发外链的过程中，绝大部分的外链是发在百度贴吧、百度知道以及百度经验和百度文库上。因为高权重的外链，是实现新站秒收的一个重要因素。再者你可以一边配合友情链接的使用，一个高质的友链可以给你的新站导入不少的权重和流量以及蜘蛛爬取的密度，那么友链我们需要注意的是链你的站没有没降权，这方面你可以参考PR或者快照更新时间，挂友链的数量密度不可太高，一般来说1天1个，然后累积到一定数量即可。这里说下原因，这是因为短时间内百度发现你增加了大量的友链，它也会认为也是从链接工厂购买来的。所以加入你购买了很多友链，不要一天就挂上去。再者就是挂上去后我们还需经常查看，配合日常点击，充分发挥友链对你网站的影响。还要对一些不友好的友链及时撤掉，打个比方说你的友链里有一个被百度K了，那么你的站也会受牵连。&lt;/p&gt;

&lt;p&gt;终上所述,我们大概知道了百度的一个收录原理，那么新站的站长也可以按以上方式去做。这也是我做了很多个站的出来的一点经验，那么怎么做呢，好的，那我们就讲一下网上流传的让百度5分钟秒收的做法：&lt;/p&gt;

&lt;p&gt;一、在还没有完善整站的时候，增删篇幅较大的时候，要注意设置robots的屏蔽各种蜘蛛，以免搜索引擎误认为你的站不稳定。&lt;/p&gt;

&lt;p&gt;　 二、新站完成后需要对站内进行相关的检查。例如关键词的密度是否合理、有没有存在死链、是否有百度的禁词，这些都是百度作为判定的一个标准，一定要引起重视。&lt;/p&gt;

&lt;p&gt;　 三、内容要尽量丰富，最好是能有若干篇原创文章，假如实在没有，呵呵，弄几遍伪原创也是可以的。&lt;/p&gt;

&lt;p&gt;　 四、用配置较好的主机或空间，保证打开的速度，完全加载成功的时间不超过5秒。&lt;/p&gt;

&lt;p&gt;　 五、以上是做站内优化需要注意的一些问题，做完了我们就可以撤掉robots的屏蔽，进行蜘蛛的吸引。吸引蜘蛛要去可以秒收的网站吸引蜘蛛，只有这样才能实现网站的秒收。至于秒收的网站有很多,像网易论坛、新浪博客、天涯社区、seowhy、A5等。&lt;/p&gt;

&lt;p&gt;　　
 以上就是实现百度5分钟秒收的方法，新手只要照着做，一定能实现快速收录，这里再提及一下极少见的情况，就是有些站过了一个月的审核期仍然没有被收录，这种情况就要考虑一下你的域名是否之前被注册过并且被百度K过，百度对于K过的域名非常严格永不收录，那么如何判断域名是否被K过，方法是这样的，可以在百度输入：stie:你的域名和domain:你的域名，如果site无记录，而domain却出现很多的记录，那么极有可能是被K过的。另外还可以通过Alexa历史排名和Whois信息查询域名是否被用过。&lt;/p&gt;

&lt;p&gt;本篇文章转自：&lt;a href=&quot;http://www.heimaoseojishu.com/forum.php?mod=viewthread&amp;amp;tid=29&amp;amp;fromuid=697&quot;&gt;新站如何让百度快速收录-黑帽SEO技术网原创文章&lt;/a&gt;
(出处: 黑帽SEO技术网)&lt;/p&gt;
</description>
				<pubDate>Mon, 20 Apr 2015 00:00:00 +0800</pubDate>
				<link>/seo/2015/04/20/baidu-include.html</link>
				<guid isPermaLink="true">/seo/2015/04/20/baidu-include.html</guid>
			</item>
		
			<item>
				<title>使用puppet去部署openstack</title>
				<description>&lt;p&gt;使用puppet去部署openstack&lt;/p&gt;

&lt;h2 id=&quot;puppet&quot;&gt;puppet简介&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Puppet是一个IT基础设施自动化管理工具，帮助系统管理员管理基础设施整个生命周期的供应(provisioning),配置(configuration)，联动(orchestration)及报告(reporting)。&lt;/li&gt;
  &lt;li&gt;基于puppet可以实现自动化重复任务，快速部署关键性应用以及在本地或者云端完成主动管理变更和快速部署扩展架构。&lt;/li&gt;
  &lt;li&gt;当然，对于管理员它是抽象的，只依赖于facter工具（可以检索出不同系统的基本信息）和ruby语言环境（主要以模版的形式来开发）。&lt;/li&gt;
  &lt;li&gt;Puppet能管理40多种资源，架构模式是master/agent管理与被管理的关系并非主从关系。
## puppet工作原理 ##&lt;/li&gt;
  &lt;li&gt;（1）客户端Puppetd向Master发起认证请求，或使用带签名的证书。&lt;/li&gt;
  &lt;li&gt;（2）Master告诉Client你是合法的。&lt;/li&gt;
  &lt;li&gt;（3）客户端Puppetd调用Facter，Facter探测出主机的一些变量，例如主机名、内存大小、IP地址等。Puppetd将这些信息通过SSL连接发送到服务器端。&lt;/li&gt;
  &lt;li&gt;（4）服务器端的PuppetMaster检测客户端的主机名，然后找到manifest对应的node配置，并对该部分内容进行解析。Facter送过来的信息可以作为变量处理，node牵涉到的代码才解析，其他没牵涉的代码不解析。解析分为几个阶段，首先是语法检查，如果语法错误就报错;如果语法没错，就继续解析，解析的结果生成一个中间的“伪代码”(catelog)，然后把伪代码发给客户端。&lt;/li&gt;
  &lt;li&gt;（5）客户端接收到“伪代码”，并且执行。&lt;/li&gt;
  &lt;li&gt;（6）客户端在执行时判断有没有File文件，如果有，则向fileserver发起请求。&lt;/li&gt;
  &lt;li&gt;（7）客户端判断有没有配置Report，如果已配置，则把执行结果发送给服务器。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;（8）服务器端把客户端的执行结果写入日志，并发送给报告系统。
## 认证过程 ##
 任何一台puppet客户端想要获取服务器的配置信息第一步必须先要认证完毕。具体认证过程如下：&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;（1）puppet客户端请求节点配置。&lt;/li&gt;
  &lt;li&gt;（2）puppetmaster进行SSL认证。&lt;/li&gt;
  &lt;li&gt;（3）将信息日志写入cactlog。&lt;/li&gt;
  &lt;li&gt;（4）puppet解析器解释代码（包括错误检查以及语法检查）,并写入日志。&lt;/li&gt;
  &lt;li&gt;（5）客户端发送日志。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;（6）ssl认证授权结束，整个流程结束。
## puppet目录结构 ##
puppet.conf主配置文件&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;fileserver.conf允许访问的文件控制&lt;/li&gt;
  &lt;li&gt;auth.conf访问权限配置&lt;/li&gt;
  &lt;li&gt;autosign.conf自动验证配置文件&lt;/li&gt;
  &lt;li&gt;manifests文件存储目录(puppet会先读取该目录的.pp文件&lt;site.pp&gt;)&lt;/site.pp&gt;&lt;/li&gt;
  &lt;li&gt;site.conf全局配置文件&lt;/li&gt;
  &lt;li&gt;modules定义模块&lt;/li&gt;
  &lt;li&gt;templates模块配置目录&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;应用背景：&lt;/h2&gt;
&lt;p&gt;目前puppet官方有根据openstack各种环境编辑好了一整套部署脚本，这一套也已经上传到社区（比如github）给一些相关人士进行维护并更新。我们只要获取那些脚本，在根据我们自己的需求来封装自己的一个类，这个类就是关于怎么调用那些人家已经完善得非常好的各种相关openstack组件模块。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作系统：centos6.5&lt;/li&gt;
  &lt;li&gt;Openstack版本：havana icehouse juno&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Puppet版本：2.7或以上（如果一些脚本需要的话可能要把版本更新到3.x以上的）
## 安装puppet ##
  设置源：
  rpm -ivh http://yum.puppetlabs.com/puppetlabs-release-el-6.noarch.rpm
  安装：
  Agent端：yum install puppet
  Server端：yum install puppet-server
  获取脚本：
  puppet module install puppetlabs-openstack
  下载git上面的脚本：
  cd /etc/puppet/modules
  git clone git://github.com/stackforge/puppet-openstack.git openstack
  cd openstack
  gem install librarian-puppet
  librarian-puppet install –path ../
## openstack模块 ##
  [root@puppet modules]# tree -L 1
  .
  ├── 99cloud
  ├── apache
  ├── ceilometer
  ├── certmonger
  .
  .
  .
  ├── vlan
  ├── vswitch
  └── xinetd&lt;/p&gt;

    &lt;p&gt;50 directories, 0 files&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在这50个左右的模块里面控制整个openstack流程安装部署的就在openstack这个模块里面，
在openstack这个模块里面主要是把各个openstack各个组件模块调用起来最后形成了可以灵活地运用各种参数去部署openstack的脚本。这是由于openstack有很多配置文件以及后面要更变配置，所有才有了如此之多的参数。当然，你可以根据自己的需要，比如一些组件你不安装或者一些配置是不需要的，然后自己组成一个模块之类的。
## 部署 ##
在自己脚本完成后并测试没有什么语法错误就可以开始部署安装，当然在这之前肯定要经过大量的测试来完善自己的脚本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;There are two kinds of use: 
---1 
	stand-alone mode: direct calling scripts
	allinone #sudo puppet apply -e &quot;class { 99cloud::allinone: }&quot;
	controller   #sudo puppet apply -e &quot;class { 99cloud::controller: }&quot;
	only compute #sudo puppet apply -e &quot;class { 99cloud::compute: }&quot;
---2
	server-agent mode:
	a: pick out one end as a service
	b: edit /etc/puppet/manifests/site.pp (If not,please create)
	c: depending on the fqdn(hostname+domain) to deploy different nodes
For your controller node, you need to assign your node the controller role. For example:
```
node 'control.localdomain' { 
	include 99cloud::controller
}
```
allinone:
include 99cloud::allinone
```
compute:
	include 99cloud::compute
```
 d: on your node exec:
 #puppet agent -t --server 'service of fqdn' 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是说主要有2种部署方式，一种是单机模式，另一种是C/S模式。如果你的服务器有上几十台以上就需要用到C/S模式，否则尽量用单机模式来部署。这样既节省时间，又方便控制各个服务器。
从裸机到openstack平台&lt;/p&gt;

&lt;p&gt;简而言之，就是先对一台裸机进行PXE（安装系统）之后再利用puppet把openstack平台架起来&lt;/p&gt;
</description>
				<pubDate>Tue, 03 Mar 2015 00:00:00 +0800</pubDate>
				<link>/deploy/2015/03/03/puppet.html</link>
				<guid isPermaLink="true">/deploy/2015/03/03/puppet.html</guid>
			</item>
		
			<item>
				<title>Linux用DD命令测试磁盘读写速度</title>
				<description>&lt;p&gt;Linux用DD命令测试磁盘读写速度&lt;/p&gt;

&lt;p&gt;　　当然这篇文章是写给我这种小白的，高手请绕路！dd是Linux/UNIX 下的一个非常有用的命令，作用是用指定大小的块拷贝一个文件，并在拷贝的同时进行指定的转换，所以可以用来测试硬盘的读写能力~
下面直接介绍几种常见的DD命令，先看一下他的区别~&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dd bs=64k count=4k if=/dev/zero of=test
dd bs=64k count=4k if=/dev/zero of=test; sync
dd bs=64k count=4k if=/dev/zero of=test conv=fdatasync
dd bs=64k count=4k if=/dev/zero of=test oflag=dsync
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这四条DD命令区别在于内存中写缓存的处理方式。&lt;/p&gt;

&lt;h2 id=&quot;dd-bs64k-count4k-ifdevzero-oftest&quot;&gt;1.dd bs=64k count=4k if=/dev/zero of=test&lt;/h2&gt;
&lt;p&gt;　　没有加任何参数，dd默认的方式不包括“同步(sync)”命令。也就是说，dd命令完成前并没有让系统真正把文件写到磁盘上。所以以上命令只是单纯地把这128MB的数据读到内存缓冲当中（写缓存[write cache]）。所以你得到的将是一个超级快的速度。因为其实dd给你的只是读取速度，直到dd完成后系统才开始真正往磁盘上写数据，但这个速度你是看不到了。所以如果这个速度很快，没有什么作用。
实际运行结果：
268435456 bytes (268 MB) copied, 1.3529 seconds, 198 MB/s&lt;/p&gt;

&lt;h2 id=&quot;dd-bs64k-count4k-ifdevzero-oftest-sync&quot;&gt;2.dd bs=64k count=4k if=/dev/zero of=test; sync&lt;/h2&gt;
&lt;p&gt;　　和前面1中的完全一样。分号隔开的只是先后两个独立的命令。当sync命令准备开始往磁盘上真正写入数据的时候，前面dd命令已经把错误的“写入速度”值显示在屏幕上了。所以你还是得不到真正的写入速度。
实际运行结果：
268435456 bytes (268 MB) copied, 0.522815 seconds, 513 MB/s&lt;/p&gt;

&lt;h2 id=&quot;dd-bs64k-count4k-ifdevzero-oftest-convfdatasync&quot;&gt;3.dd bs=64k count=4k if=/dev/zero of=test conv=fdatasync&lt;/h2&gt;
&lt;p&gt;　　加入这个参数后，dd命令执行到最后会真正执行一次“同步(sync)”操作，所以这时候你得到的是读取这128M数据到内存并写入到磁盘上所需的时间，这样算出来的时间才是比较符合实际使用结果的。
实际运行结果：
268435456 bytes (268 MB) copied, 2.8046 seconds, 95.7 MB/s&lt;/p&gt;

&lt;h2 id=&quot;dd-bs64k-count4k-ifdevzero-oftest-oflagdsync&quot;&gt;4.dd bs=64k count=4k if=/dev/zero of=test oflag=dsync&lt;/h2&gt;
&lt;p&gt;　　加入这个参数后，dd在执行时每次都会进行同步写入操作。也就是说，这条命令每次读取64k后就要先把这64k写入磁盘，然后再读取下面这64k，一共重复128次。这可能是最慢的一种方式了，因为基本上没有用到写缓存(write cache)。
实际运行结果：
268435456 bytes (268 MB) copied, 3.40069 seconds, 78.9 MB/s
　　一般来说，第四种方法是最严格的，可以模拟数据库的插入操作，所以很慢，也是用来测试vps硬盘性能标准的一条标杆，一般来说测试结果，如果超过10M，对正常建站就无影响。超过50M，就是非常给力状态，看了这个vps硬盘性能非常的好，DD速度达到了78.9MB/s。
　　在这几条命令中，bs=64k表示同时读入/输出的块大小为64k个字节，count=4k表示拷贝块的个数为4000个，如果测试再严格一点，我们运行1G数据量的DD：
dd if=/dev/zero of=test bs=64k count=16k oflag=dsync
表示每个块大小为64k个字节，测试16k个数量的块，实际测试结果：
1073741824 bytes (1.1 GB) copied, 18.9098 seconds, 56.8 MB/s&lt;/p&gt;

</description>
				<pubDate>Sun, 01 Feb 2015 00:00:00 +0800</pubDate>
				<link>/hardware/2015/02/01/Linux-dd.html</link>
				<guid isPermaLink="true">/hardware/2015/02/01/Linux-dd.html</guid>
			</item>
		
			<item>
				<title>Git学习及下载资源</title>
				<description>&lt;p&gt;保存Git源码管理原理学习及下载链接&lt;/p&gt;

&lt;p&gt;Git学习资源&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://developer.51cto.com/art/201111/302195.htm&quot;&gt;http://developer.51cto.com/art/201111/302195.htm&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/luckarecs/article/details/7427605&quot;&gt;http://blog.csdn.net/luckarecs/article/details/7427605&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.open-open.com/lib/list/282?pn=2&quot;&gt;http://www.open-open.com/lib/list/282?pn=2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Git下载链接&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://msysgit.github.com/&quot;&gt;http://msysgit.github.com/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://code.google.com/p/gitextensions/downloads/list&quot;&gt;http://code.google.com/p/gitextensions/downloads/list&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://gitscc.codeplex.com/&quot;&gt;http://gitscc.codeplex.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Wed, 21 Jan 2015 00:00:00 +0800</pubDate>
				<link>/resource/2015/01/21/git-resource.html</link>
				<guid isPermaLink="true">/resource/2015/01/21/git-resource.html</guid>
			</item>
		
	</channel>
</rss>
